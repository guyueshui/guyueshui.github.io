<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>浅谈 Logistic 回归 - 水阙</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="Yychi"><meta name=description content="In editing&mldr;
Logistic 回归属于分类模型！！！
从最小二乘说起 线性回归 概率解释 Sigmoid 函数的引入 如果把我比作一张白纸，在我的知识储备中，现在只有线性回归。但是要处理分类问题，我该怎么办？没办法，先考虑一个二分类问题，$y \in {0,1}$，我们准备霸王硬上弓，用回归模型套上去！ $$ y = h_{\theta}(x) $$
"><meta name=keywords content="水阙,yychi"><meta name=generator content="Hugo 0.149.0 with theme even"><link rel=canonical href=https://guyueshui.github.io/post/%E6%B5%85%E8%B0%88-logistic-%E5%9B%9E%E5%BD%92/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><link href=/sass/main.min.d70690ffa8f6f6f22c0e62cb71d66e67705dd030e9bcb57e066e4adb9823988c.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><link rel=stylesheet href=/css/even-custom.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&family=Noto+Serif+SC:wght@400;500;700&display=swap" rel=stylesheet><meta property="og:url" content="https://guyueshui.github.io/post/%E6%B5%85%E8%B0%88-logistic-%E5%9B%9E%E5%BD%92/"><meta property="og:site_name" content="水阙"><meta property="og:title" content="浅谈 Logistic 回归"><meta property="og:description" content="In editing…
Logistic 回归属于分类模型！！！
从最小二乘说起 线性回归 概率解释 Sigmoid 函数的引入 如果把我比作一张白纸，在我的知识储备中，现在只有线性回归。但是要处理分类问题，我该怎么办？没办法，先考虑一个二分类问题，$y \in {0,1}$，我们准备霸王硬上弓，用回归模型套上去！ $$ y = h_{\theta}(x) $$"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-03-15T22:59:39+00:00"><meta property="article:modified_time" content="2019-03-15T22:59:39+00:00"><meta property="article:tag" content="Math"><meta property="article:tag" content="机器学习"><meta itemprop=name content="浅谈 Logistic 回归"><meta itemprop=description content="In editing…
Logistic 回归属于分类模型！！！
从最小二乘说起 线性回归 概率解释 Sigmoid 函数的引入 如果把我比作一张白纸，在我的知识储备中，现在只有线性回归。但是要处理分类问题，我该怎么办？没办法，先考虑一个二分类问题，$y \in {0,1}$，我们准备霸王硬上弓，用回归模型套上去！ $$ y = h_{\theta}(x) $$"><meta itemprop=datePublished content="2019-03-15T22:59:39+00:00"><meta itemprop=dateModified content="2019-03-15T22:59:39+00:00"><meta itemprop=wordCount content="796"><meta itemprop=keywords content="Math,机器学习"><meta name=twitter:card content="summary"><meta name=twitter:title content="浅谈 Logistic 回归"><meta name=twitter:description content="In editing…
Logistic 回归属于分类模型！！！
从最小二乘说起 线性回归 概率解释 Sigmoid 函数的引入 如果把我比作一张白纸，在我的知识储备中，现在只有线性回归。但是要处理分类问题，我该怎么办？没办法，先考虑一个二分类问题，$y \in {0,1}$，我们准备霸王硬上弓，用回归模型套上去！ $$ y = h_{\theta}(x) $$"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Yychi's Blog</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/search/><li class=mobile-menu-item>Search</li></a><a href=/links/><li class=mobile-menu-item>More</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/sketch/><li class=mobile-menu-item>Sketch</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Yychi's Blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/search/>Search</a></li><li class=menu-item><a class=menu-item-link href=/links/>More</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/sketch/>Sketch</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>浅谈 Logistic 回归</h1><div class=post-meta><span class=post-time>March 15, 2019</span><div class=post-category><a href=/categories/notes/>Notes</a></div><span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><a href=#从最小二乘说起>从最小二乘说起</a></li><li><a href=#线性回归>线性回归</a></li><li><a href=#概率解释>概率解释</a></li><li><a href=#sigmoid-函数的引入>Sigmoid 函数的引入</a></li><li><a href=#logistic-回归>Logistic 回归</a></li><li><a href=#扩展为-softmax>扩展为 softmax</a></li></ul></nav></div></div><div class=post-content><p>In editing&mldr;</p><p><strong>Logistic 回归属于分类模型！！！</strong></p><h2 id=从最小二乘说起>从最小二乘说起</h2><h2 id=线性回归>线性回归</h2><h2 id=概率解释>概率解释</h2><h2 id=sigmoid-函数的引入>Sigmoid 函数的引入</h2><p>如果把我比作一张白纸，在我的知识储备中，现在只有线性回归。但是要处理分类问题，我该怎么办？没办法，先考虑一个二分类问题，$y \in {0,1}$，我们准备霸王硬上弓，用回归模型套上去！
$$
y = h_{\theta}(x)
$$</p><p>至少我们希望$h_{\theta}(x) \in (0,1)$，就那么刚刚好，有一族函数，这里我们特指其中一个
$$
g(z) = \frac{1}{1+e^{-z}} \in (0,1)
$$</p><p>请记住它的名字，它就是大名鼎鼎的 sigmoid 函数。可以的话，请再记住它两个迷人的性质：</p><ol><li>$g&rsquo;(t) = g(t)(1-g(t))$</li><li>$1 - g(t) = g(-t)$</li></ol><h2 id=logistic-回归>Logistic 回归</h2><p>现在，我们模型的假设是</p><p>$$
\begin{split}
y &= h_{\mathbf{\theta}}(\mathrm{x}) = g(\mathbf{\theta}^T \mathrm{x}) \
&= \frac{1}{1+\exp({-\theta^T \mathrm{x}})} = g(z)
\end{split}
$$</p><p>我们希望通过训练改变 $\theta$ 的值，进一步改善我们的模型。现在，我们打算换一个角度来看待这个问题，因为$g(\theta^T x) \in (0,1)$，正好可以表示一个概率，而之前我们看到，最小二乘实际上等价于，我对数据有一些假设（高斯白噪声），在这些假设下，做参数$\theta$的极大似然估计 (MLE). 基于这个想法，我们假设，</p><p>$$
\begin{split}
P(y=1|x;\theta) &= h_{\theta}(x) \newline
P(y=0|x;\theta) &= 1 - h_{\theta}(x)
\end{split}
$$</p><p>然后就那么刚刚好，回忆一下 sigmoid 函数有哪些迷人的性质，你会发现下面的式子也是对的</p><p>$$
p(y|x;\theta) = [h_{\theta}(x)]^y [1-h_{\theta}(x)]^{1-y}
$$</p><p>再假设 m 个样本独立同分布，我们得到似然函数
$$
\begin{split}
L(\theta) &= p(\mathbf{y} | \mathbf{X}; \mathrm{\theta}) \
&= \prod_{i=1}^m p(y_i | x_i ; \theta)
\end{split}
$$</p><p>进一步，得到对数似然
$$
\begin{split}
l(\theta) &= \log L(\theta) \
&=\sum_{i=1}^m \left[ y_i\log h(x_i) + (1-y_i)\log(1-h(x_i)) \right]
\end{split}
$$</p><p>现在，我们基于 MLE 的方法，来调整参数 $\theta$ 的值，使得对数似然函数最大。很自然的，我们可以使用梯度上升的方法，更新规则为
$$
\theta = \theta + \alpha \nabla_{\theta} l(\theta)
$$</p><p>注意梯度上升是沿着正梯度方向更新。给定一个训练样本 $(x,y)$, 其梯度为</p><p>$$
\begin{split}
\frac{\partial}{\partial \theta_j} l(\theta)
&= \left(\frac{y}{g(z)} - \frac{1-y}{1-g(z)}
\right)
\frac{\partial g(z)}{\partial z} \frac{\partial z}{\partial \theta_j} \newline
&= (y-g(z)) \frac{\partial z}{\partial \theta_j} \newline
&= (y-h_{\theta}(x))x_j
\end{split}
$$</p><p>迭代使得似然函数最大化，完成训练。最后应该输出一个 0~1 之间的概率值。我们可以人为设定一个阈值（如 0.5），当输出概率大于 0.5，判定$y=1$，反之亦然。如此一来，就完成了回归到分类的转化。</p><p>另外，上述 sigmoid 函数又叫 logistic 函数，故名。<strong>Logistic 回归事实上是一个分类器！！！</strong></p><h2 id=扩展为-softmax>扩展为 softmax</h2></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Yychi</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>March 15, 2019</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/math/>math</a>
<a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></div><nav class=post-nav><a class=prev href=/post/leetcode-sum-of-two-integers/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">LeetCode: Sum of Two Integers</span>
<span class="prev-text nav-mobile">Prev</span>
</a><a class=next href=/post/%E8%B0%88%E8%B0%88-latex-%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%AD%97%E4%BD%93/><span class="next-text nav-default">快速自定义 LaTeX 排版字体</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://giscus.app/client.js data-repo=guyueshui/guyueshui.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkxNDI4MTY5NTE=" data-category=Ideas data-category-id=DIC_kwDOCIM2t84CW4nN data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:guyueshui002@gmail.com class="iconfont icon-email" title=email></a><a href=https://github.com/guyueshui class="iconfont icon-github" title=github></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer></div><span class=copyright-year>&copy;
2018 -
2025<span class=heart><i class="iconfont icon-heart"></i></span><span>Yychi</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>